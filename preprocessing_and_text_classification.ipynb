{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4gYJOhFJjVlw",
        "RFSNLIPCLEt_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "4gYJOhFJjVlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install torch\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX1eR6StDhzG",
        "outputId": "c07edddc-8981-4bec-bf7a-89c84f53f0ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.3 (from hazm)\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1 (from hazm)\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394472 sha256=9362d3ab0b1baaff10e08426cdb75178210b732b1e2c46a8cfbbe357af71e85b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=180334 sha256=16146904150d80a014b56233e8fe38773033e4d31b645df022fe02a38d4e8f1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# مجموعه داده‌ها\n",
        "sentences = [\n",
        "    \"جمله اول برای آموزش مدل.\",\n",
        "    \"جمله دوم برای آموزش مدل.\",\n",
        "    \"جمله سوم برای آموزش مدل.\"\n",
        "]\n",
        "\n",
        "# توکن‌بندی جملات\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# آموزش مدل Word2Vec\n",
        "embedding_dim = 100  # ابعاد بردار تعبیه\n",
        "model = Word2Vec(tokenized_sentences, vector_size=embedding_dim, min_count=1)\n",
        "\n",
        "# تست واژگان\n",
        "word = \"جمله\"\n",
        "if word in model.wv.key_to_index:\n",
        "    word_embedding = model.wv[word]\n",
        "    print(f\"بردار تعبیه واژه '{word}': {word_embedding}\")\n",
        "else:\n",
        "    print(f\"واژه '{word}' در مدل وجود ندارد.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLt1FhT7HU_f",
        "outputId": "dd8accf7-fc30-4e50-9082-c625e5165e40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بردار تعبیه واژه 'جمله': [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
            "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
            " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
            "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
            " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
            "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
            " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
            " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
            "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
            " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
            "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
            "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
            " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
            "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
            " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
            "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
            "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRjIsA1BDEuh",
        "outputId": "3accfd94-0391-4609-bdb3-fc7e283484ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "متن نمونه برا پیش‌پرداز .\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "       dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1b0441d6cf39>:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  text_tensor = torch.tensor(text_embedding)\n"
          ]
        }
      ],
      "source": [
        "from hazm import Normalizer, word_tokenize, Stemmer, Lemmatizer, stopwords_list\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# تنظیمات پیش‌پردازش\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "stopwords = set(stopwords_list())\n",
        "\n",
        "# تابع پیش‌پردازش\n",
        "def preprocess_text(text):\n",
        "    # تمیزکاری متن\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "\n",
        "    # توکن‌بندی\n",
        "    tokens = word_tokenize(normalized_text)\n",
        "\n",
        "    # عصاره‌گیری و حذف کلمات توقف\n",
        "    preprocessed_tokens = []\n",
        "    for token in tokens:\n",
        "        stemmed_token = stemmer.stem(token)\n",
        "        lemmatized_token = lemmatizer.lemmatize(token)\n",
        "        if stemmed_token not in stopwords:\n",
        "            preprocessed_tokens.append(stemmed_token)\n",
        "\n",
        "    # تبدیل به متن پیش‌پردازش شده\n",
        "    preprocessed_text = ' '.join(preprocessed_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# متن نمونه\n",
        "text = \"متن نمونه برای پیش‌پردازش.\"\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "preprocessed_text = preprocess_text(text)\n",
        "\n",
        "# چاپ متن پیش‌پردازش شده\n",
        "print(preprocessed_text)\n",
        "\n",
        "# تبدیل متن به بردار تعبیه شده\n",
        "embedding_dim = 20  # ابعاد بردار تعبیه\n",
        "embedding = np.zeros(embedding_dim)\n",
        "\n",
        "# در اینجا باید بردار تعبیه شده متناظر هر کلمه را از مدل خود بدست آورید\n",
        "# به عنوان مثال، اینجا بردار صفر به عنوان بردار تعبیه استفاده شده است\n",
        "text_embedding = [embedding] * len(preprocessed_text.split())\n",
        "\n",
        "# تبدیل به تنسور پایتورچ\n",
        "text_tensor = torch.tensor(text_embedding)\n",
        "\n",
        "# چاپ تنسور متن\n",
        "print(text_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# مجموعه داده‌ها\n",
        "sentences = [\n",
        "    \"جمله اول برای پیش‌پردازش.\",\n",
        "    \"جمله دوم برای پیش‌پردازش.\",\n",
        "    \"جمله سوم برای پیش‌پردازش.\"\n",
        "]\n",
        "\n",
        "# توکن‌بندی جملات\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# آموزش مدل Word2Vec\n",
        "embedding_dim = 100  # ابعاد بردار تعبیه\n",
        "model = Word2Vec(tokenized_sentences, vector_size=embedding_dim, min_count=1)\n",
        "model.save(\"/content/word2vec.bin\")\n",
        "# تابع پیش‌پردازش\n",
        "def preprocess_text(text):\n",
        "    # توکن‌بندی\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # بردارهای تعبیه واژه‌ها\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv.key_to_index:\n",
        "            embeddings.append(model.wv[token])\n",
        "    \n",
        "    # تبدیل به آرایه NumPy\n",
        "    embeddings = np.array(embeddings)\n",
        "    \n",
        "    # میانگین بردارها\n",
        "    averaged_embedding = np.mean(embeddings, axis=0)\n",
        "    \n",
        "    return averaged_embedding\n",
        "\n",
        "# داده نمونه\n",
        "text = \"متن نمونه برای پیش‌پردازش.\"\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "preprocessed_text = preprocess_text(text)\n",
        "\n",
        "# چاپ متن پیش‌پردازش شده\n",
        "print(preprocessed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH-LoFYEIATU",
        "outputId": "f01a26ca-78ab-444e-d6bb-3c2fca4097c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.0204502e-03  2.3265027e-03  1.1601817e-03  4.4585788e-03\n",
            "  1.9441992e-03 -1.9793429e-03  1.2970553e-03  5.8876630e-03\n",
            " -5.3908867e-03 -1.2437898e-03  7.7764346e-04 -4.3555154e-03\n",
            " -2.7507017e-04  4.8439936e-03  1.9943765e-03 -2.2148299e-04\n",
            "  4.9457885e-03  6.1485004e-03 -6.8439380e-03 -5.0487821e-03\n",
            "  8.6508709e-04 -1.4371508e-03  3.7892789e-03 -5.8771838e-03\n",
            "  7.0237289e-03 -1.7783059e-03  8.4814453e-04  5.1431214e-03\n",
            " -5.2704266e-03  2.2443922e-03  2.6411507e-03 -4.2396891e-03\n",
            "  5.1100488e-04 -5.1633506e-03 -2.3695752e-03 -2.4423216e-04\n",
            "  4.8550260e-03  1.1566350e-03  3.6751830e-03  1.2127521e-03\n",
            " -2.4331918e-03 -8.3944760e-05 -6.2709427e-03  6.8205176e-04\n",
            "  2.7955804e-04  4.0425453e-03 -9.6116994e-05  1.7897360e-03\n",
            "  7.0872699e-04  3.3303676e-03  3.6556335e-04 -2.5534711e-03\n",
            " -3.8054781e-03 -1.1705410e-03 -5.2073196e-04 -6.7187037e-04\n",
            "  3.8936157e-03 -1.5247840e-03 -3.5736070e-03  4.4638556e-03\n",
            " -1.9409339e-03 -7.0196234e-05 -1.3346660e-04 -3.7458800e-03\n",
            " -1.7202515e-03  5.7579973e-03  4.8375824e-03  1.7534905e-03\n",
            " -7.1950158e-04  4.9027591e-03  1.9183708e-03  1.5952379e-03\n",
            "  2.7465515e-03 -2.5287332e-04  1.3760664e-04  7.3860883e-04\n",
            "  3.3227410e-03  1.6076128e-04 -1.1072432e-04 -4.1370397e-03\n",
            " -3.0025968e-03 -6.0169166e-04  6.8730378e-04  1.3102528e-04\n",
            " -1.0857560e-03 -1.9864913e-03  4.6265451e-04 -7.3425602e-03\n",
            "  2.6378043e-03  7.2294055e-04 -5.9676834e-04  1.4384780e-03\n",
            "  4.9501280e-03 -3.9212322e-03  4.9554580e-03  4.3664235e-03\n",
            "  7.6948386e-04 -2.9734939e-03 -1.3183434e-04  1.2931094e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# مجموعه داده‌ها\n",
        "sentences = [\n",
        "    \"جمله اول برای پیش‌پردازش.\",\n",
        "    \"جمله دوم برای پیش‌پردازش.\",\n",
        "    \"جمله سوم برای پیش‌پردازش.\"\n",
        "]\n",
        "\n",
        "# توکن‌بندی جملات\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# آموزش مدل Word2Vec\n",
        "embedding_dim = 100  # ابعاد بردار تعبیه\n",
        "model = Word2Vec(tokenized_sentences, vector_size=embedding_dim, min_count=1)\n",
        "model.save(\"/content/word2vec.bin\")\n",
        "\n",
        "# تابع پیش‌پردازش\n",
        "def preprocess_text(text):\n",
        "    # توکن‌بندی\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # بردارهای تعبیه واژه‌ها\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv.key_to_index:\n",
        "            embeddings.append(model.wv[token])\n",
        "    \n",
        "    # تبدیل به آرایه NumPy\n",
        "    embeddings = np.array(embeddings)\n",
        "    \n",
        "    # میانگین بردارها\n",
        "    averaged_embedding = np.mean(embeddings, axis=0)\n",
        "    \n",
        "    return averaged_embedding\n",
        "\n",
        "# تابع بازگرداندن به متن\n",
        "def get_original_text(embedding):\n",
        "    # جستجوی نزدیک‌ترین بردار تعبیه به بردار ورودی\n",
        "    similar_word = model.wv.similar_by_vector(embedding, topn=1)[0][0]\n",
        "    \n",
        "    return similar_word\n",
        "\n",
        "# داده نمونه\n",
        "text = \"متن نمونه برای پیش‌پردازش.\"\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "preprocessed_text = preprocess_text(text)\n",
        "\n",
        "# بازگرداندن به متن اصلی\n",
        "original_text = get_original_text(preprocessed_text)\n",
        "\n",
        "# چاپ متن اصلی\n",
        "print(preprocessed_text)\n",
        "\n",
        "# چاپ متن اصلی\n",
        "print(original_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13bKO2K9IPat",
        "outputId": "646f20fe-8fb1-4c76-f2f4-e209ac3d35b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.0204502e-03  2.3265027e-03  1.1601817e-03  4.4585788e-03\n",
            "  1.9441992e-03 -1.9793429e-03  1.2970553e-03  5.8876630e-03\n",
            " -5.3908867e-03 -1.2437898e-03  7.7764346e-04 -4.3555154e-03\n",
            " -2.7507017e-04  4.8439936e-03  1.9943765e-03 -2.2148299e-04\n",
            "  4.9457885e-03  6.1485004e-03 -6.8439380e-03 -5.0487821e-03\n",
            "  8.6508709e-04 -1.4371508e-03  3.7892789e-03 -5.8771838e-03\n",
            "  7.0237289e-03 -1.7783059e-03  8.4814453e-04  5.1431214e-03\n",
            " -5.2704266e-03  2.2443922e-03  2.6411507e-03 -4.2396891e-03\n",
            "  5.1100488e-04 -5.1633506e-03 -2.3695752e-03 -2.4423216e-04\n",
            "  4.8550260e-03  1.1566350e-03  3.6751830e-03  1.2127521e-03\n",
            " -2.4331918e-03 -8.3944760e-05 -6.2709427e-03  6.8205176e-04\n",
            "  2.7955804e-04  4.0425453e-03 -9.6116994e-05  1.7897360e-03\n",
            "  7.0872699e-04  3.3303676e-03  3.6556335e-04 -2.5534711e-03\n",
            " -3.8054781e-03 -1.1705410e-03 -5.2073196e-04 -6.7187037e-04\n",
            "  3.8936157e-03 -1.5247840e-03 -3.5736070e-03  4.4638556e-03\n",
            " -1.9409339e-03 -7.0196234e-05 -1.3346660e-04 -3.7458800e-03\n",
            " -1.7202515e-03  5.7579973e-03  4.8375824e-03  1.7534905e-03\n",
            " -7.1950158e-04  4.9027591e-03  1.9183708e-03  1.5952379e-03\n",
            "  2.7465515e-03 -2.5287332e-04  1.3760664e-04  7.3860883e-04\n",
            "  3.3227410e-03  1.6076128e-04 -1.1072432e-04 -4.1370397e-03\n",
            " -3.0025968e-03 -6.0169166e-04  6.8730378e-04  1.3102528e-04\n",
            " -1.0857560e-03 -1.9864913e-03  4.6265451e-04 -7.3425602e-03\n",
            "  2.6378043e-03  7.2294055e-04 -5.9676834e-04  1.4384780e-03\n",
            "  4.9501280e-03 -3.9212322e-03  4.9554580e-03  4.3664235e-03\n",
            "  7.6948386e-04 -2.9734939e-03 -1.3183434e-04  1.2931094e-03]\n",
            "پیش‌پردازش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text classification**"
      ],
      "metadata": {
        "id": "RFSNLIPCLEt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from hazm import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# تنظیمات پیش‌پردازش\n",
        "embedding_dim = 100  # ابعاد بردار تعبیه\n",
        "model_path = '/content/word2vec.bin'  # مسیر مدل Word2Vec ذخیره شده\n",
        "\n",
        "# تابع پیش‌پردازش\n",
        "def preprocess_text(text):\n",
        "    # بارگیری مدل Word2Vec\n",
        "    model = Word2Vec.load(model_path)\n",
        "    \n",
        "    # توکن‌بندی\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # بردارهای تعبیه واژه‌ها\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv.key_to_index:\n",
        "            embeddings.append(model.wv[token])\n",
        "    \n",
        "    # تبدیل به آرایه NumPy\n",
        "    embeddings = np.array(embeddings)\n",
        "    \n",
        "    # میانگین بردارها\n",
        "    averaged_embedding = np.mean(embeddings, axis=0)\n",
        "    \n",
        "    # تبدیل به Tensor PyTorch\n",
        "    tensor_embedding = torch.Tensor(averaged_embedding)\n",
        "    \n",
        "    return tensor_embedding\n",
        "\n",
        "# تعریف معماری مدل کلاس‌بندی\n",
        "class ClassifierModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# تعریف مجموعه داده\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n",
        "# تنظیمات آموزش\n",
        "input_dim = embedding_dim  # تعداد ویژگی‌ها\n",
        "hidden_dim = 50  # ابعاد لایه مخفی\n",
        "output_dim = 2  # تعداد کلاس‌ها\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# بارگیری مدل Word2Vec\n",
        "model = Word2Vec.load(model_path)\n",
        "\n",
        "# مجموعه داده‌ها\n",
        "texts = [\n",
        "    \"جمله اول برای پیش‌پردازش.\",\n",
        "    \"جمله دوم برای پیش‌پردازش.\",\n",
        "    \"جمله سوم برای پیش‌پردازش.\"\n",
        "]\n",
        "labels=[\n",
        "    0,1,0\n",
        "]\n",
        "\n",
        "# تابع پیش‌پردازش داده‌ها\n",
        "preprocessed_data = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# تبدیل برچسب‌ها به Tensor PyTorch\n",
        "labels = torch.Tensor(labels)\n",
        "\n",
        "# اندازه داده‌ها و برچسب‌ها\n",
        "data_size = len(preprocessed_data)\n",
        "\n",
        "# اندازه مجموعه‌های آموزش و اعتبارسنجی\n",
        "train_size = int(0.8 * data_size)\n",
        "val_size = data_size - train_size\n",
        "\n",
        "# تقسیم داده‌ها به مجموعه‌های آموزش و اعتبارسنجی\n",
        "train_data = preprocessed_data[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "val_data = preprocessed_data[train_size:]\n",
        "val_labels = labels[train_size:]\n",
        "\n",
        "# تبدیل داده‌ها به شیوه دیتاست سفارشی\n",
        "train_dataset = CustomDataset(train_data, train_labels)\n",
        "val_dataset = CustomDataset(val_data, val_labels)\n",
        "\n",
        "# ساخت لودر داده برای آموزش و اعتبارسنجی\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# تعریف مدل\n",
        "model = ClassifierModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# تعریف تابع هزینه و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# آموزش مدل\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_correct = 0\n",
        "    val_correct = 0\n",
        "    \n",
        "    # حالت آموزش\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    # حالت اعتبارسنجی\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            \n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    # محاسبه معدل دقت و خطا\n",
        "    train_loss = train_loss / train_size\n",
        "    val_loss = val_loss / val_size\n",
        "    train_accuracy = train_correct / train_size\n",
        "    val_accuracy = val_correct / val_size\n",
        "    \n",
        "    # چاپ نتایج\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    # چاپ نتایج\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}')\n",
        "    print('-----------------------')\n",
        "\n",
        "    # ذخیره‌سازی مدل\n",
        "    torch.save(model.state_dict(), '/content/model.pt')"
      ],
      "metadata": {
        "id": "R7DMkF-ZLTQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d65d20-93c4-4876-e9cb-a0a6f0cfabc4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Epoch 1/10\n",
            "Train Loss: 0.6936 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6669 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 2/10\n",
            "Epoch 2/10\n",
            "Train Loss: 0.6935 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6689 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 3/10\n",
            "Epoch 3/10\n",
            "Train Loss: 0.6934 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6708 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 4/10\n",
            "Epoch 4/10\n",
            "Train Loss: 0.6933 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6726 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 5/10\n",
            "Epoch 5/10\n",
            "Train Loss: 0.6932 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6745 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 6/10\n",
            "Epoch 6/10\n",
            "Train Loss: 0.6931 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6763 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 7/10\n",
            "Epoch 7/10\n",
            "Train Loss: 0.6931 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6781 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 8/10\n",
            "Epoch 8/10\n",
            "Train Loss: 0.6930 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6799 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 9/10\n",
            "Epoch 9/10\n",
            "Train Loss: 0.6929 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6816 | Val Accuracy: 1.0000\n",
            "-----------------------\n",
            "Epoch 10/10\n",
            "Epoch 10/10\n",
            "Train Loss: 0.6928 | Train Accuracy: 0.5000\n",
            "Val Loss: 0.6832 | Val Accuracy: 1.0000\n",
            "-----------------------\n"
          ]
        }
      ]
    }
  ]
}